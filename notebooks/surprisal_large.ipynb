{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from eff.data.dataset import CLTSDataset\n",
    "from eff.train import get_class_weights_balanced, get_train_test_valid_split\n",
    "from eff.train.dataset import TrainDataset, UnmaskedTestSet, \\\n",
    "    ConsonantMaskingTestSet, VowelMaskingTestSet\n",
    "from eff.train.scripts import train, test\n",
    "from eff.util import constants\n",
    "from eff.util.util import save_results\n",
    "\n",
    "from eff.data import load_ipa_transcriptions\n",
    "from eff.data.dataset import CLTSDataset\n",
    "\n",
    "\n",
    "json_path_fin = Path(\"../../data/en-wikt/kaikki.org-dictionary-Finnish-all-non-inflected-senses.json\")\n",
    "json_path_tur = Path(\"../../data/en-wikt/kaikki.org-dictionary-Turkish-all-non-inflected-senses.json\")\n",
    "# Too few IPA-transcribed items for Estonian\n",
    "# json_path_ekk = Path(\"../../data/en-wikt/kaikki.org-dictionary-Estonian-all-non-inflected-senses.json\")\n",
    "json_path_arb = Path(\"../../data/en-wikt/kaikki.org-dictionary-Arabic-all-non-inflected-senses.json\")\n",
    "json_path_hye = Path(\"../../data/en-wikt/kaikki.org-dictionary-Armenian-all-non-inflected-senses.json\")\n",
    "base_path = Path(\"./out/wikt_unique\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ipa_trans_fin = load_ipa_transcriptions(json_path_fin)\n",
    "ipa_trans_tur = load_ipa_transcriptions(json_path_tur)\n",
    "ipa_trans_hye = load_ipa_transcriptions(json_path_hye)\n",
    "ipa_trans_arb = load_ipa_transcriptions(json_path_arb)\n",
    "\n",
    "print(len(ipa_trans_fin))\n",
    "print(len(ipa_trans_tur))\n",
    "print(len(ipa_trans_hye))\n",
    "print(len(ipa_trans_arb))\n",
    "\n",
    "N = min(len(ipa_trans_fin), len(ipa_trans_tur), len(ipa_trans_hye), len(ipa_trans_arb))\n",
    "print(\"N =\", N)\n",
    "clts_ds_fin = CLTSDataset(ipa_trans_fin[:N], unique_sequences=True)\n",
    "clts_ds_tur = CLTSDataset(ipa_trans_tur[:N], unique_sequences=True)\n",
    "clts_ds_hye = CLTSDataset(ipa_trans_hye[:N], unique_sequences=True)\n",
    "clts_ds_arb = CLTSDataset(ipa_trans_arb[:N], unique_sequences=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "76714\n",
      "4829\n",
      "14972\n",
      "7821\n",
      "N = 4829\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = dict(\n",
    "    fin=clts_ds_fin, tur=clts_ds_tur,\n",
    "    hye=clts_ds_hye, arb=clts_ds_arb\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "batch_size = 32\n",
    "n_layers = 1\n",
    "embedding_size = 64\n",
    "hidden_size = 256\n",
    "dropout = 0.33\n",
    "patience = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# %%capture log\n",
    "\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from importlib import reload\n",
    "\n",
    "from eff.model import lstm\n",
    "LstmLM = reload(lstm)\n",
    "from eff.train import generate_batch\n",
    "\n",
    "datasets = defaultdict(lambda: defaultdict(lambda: {}))\n",
    "res = defaultdict(lambda: defaultdict(lambda: {}))\n",
    "models = {}\n",
    "criteria = {}\n",
    "\n",
    "for lang_id, clts_dataset in data.items():\n",
    "    print(lang_id)\n",
    "    datasets[lang_id]['clts'] = clts_dataset\n",
    "    train_words, valid_words, test_words = get_train_test_valid_split(clts_dataset.words, \\\n",
    "        test_size=0.3, valid_size=0.1)\n",
    "\n",
    "    train_set = TrainDataset(\n",
    "                    words=train_words,\n",
    "                    input_alphabet=clts_dataset.input_alphabet,\n",
    "                    output_alphabet=clts_dataset.output_alphabet,\n",
    "                    bipa=clts_dataset.bipa,\n",
    "                    masking=0.25\n",
    "                )\n",
    "    valid_set = TrainDataset( \n",
    "                    words=valid_words,\n",
    "                    input_alphabet=clts_dataset.input_alphabet,\n",
    "                    output_alphabet=clts_dataset.output_alphabet,\n",
    "                    bipa=clts_dataset.bipa, \n",
    "                    masking=0.25\n",
    "                )\n",
    "    test_set = UnmaskedTestSet(\n",
    "                    words=test_words,\n",
    "                    input_alphabet=clts_dataset.input_alphabet,\n",
    "                    output_alphabet=clts_dataset.output_alphabet,\n",
    "                    bipa=clts_dataset.bipa\n",
    "                )\n",
    "    \n",
    "    test_set_vowel = VowelMaskingTestSet(\n",
    "                        words=test_words,\n",
    "                        input_alphabet=clts_dataset.input_alphabet,\n",
    "                        output_alphabet=clts_dataset.output_alphabet,\n",
    "                        bipa=clts_dataset.bipa\n",
    "                    )\n",
    "\n",
    "    test_set_consonant = ConsonantMaskingTestSet(\n",
    "                            words=test_words,\n",
    "                            input_alphabet=clts_dataset.input_alphabet,\n",
    "                            output_alphabet=clts_dataset.output_alphabet,\n",
    "                            bipa=clts_dataset.bipa\n",
    "                        )\n",
    "\n",
    "    datasets[lang_id]['torch']['unmasked'] = test_set\n",
    "    datasets[lang_id]['torch']['vowel_masking'] = test_set_vowel\n",
    "    datasets[lang_id]['torch']['consonant_masking'] = test_set_consonant\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    test_loader_vowel = DataLoader(test_set_vowel, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    test_loader_consonant = DataLoader(test_set_consonant, batch_size=batch_size, collate_fn=generate_batch)\n",
    "    \n",
    "    train_labels = list(itertools.chain.from_iterable([t.cpu().tolist() for t in train_set._Y]))  \n",
    "    \n",
    "    missing_labels = list(set(clts_dataset.output_alphabet.indices).difference(set(train_labels)))\n",
    "    train_labels = train_labels + [clts_dataset.output_alphabet.PAD_IDX] + missing_labels\n",
    "    weight = get_class_weights_balanced(ignore_classes=[clts_dataset.pad_idx, clts_dataset.mask_idx], \\\n",
    "        classes=clts_dataset.output_alphabet.indices, y=train_labels)\n",
    "\n",
    "    criterion = CrossEntropyLoss(weight=weight)\n",
    "    criteria[lang_id] = criterion\n",
    "    model = lstm.LstmLM(\n",
    "        input_dim=len(clts_dataset.input_alphabet),\n",
    "        output_dim=len(clts_dataset.output_alphabet),\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=256,\n",
    "        dropout=0.33,\n",
    "        n_layers=2,\n",
    "        loss_fn=criterion\n",
    "    )\n",
    "    # print(model)\n",
    "    model.to(constants.device)\n",
    "    optimizer = Adam(model.parameters())\n",
    "    \n",
    "    train(model, train_loader, valid_loader, optimizer, criterion, patience=patience)\n",
    "    \n",
    "    models[lang_id] = model\n",
    "\n",
    "    logprobs, target_indices, targets = test(model, test_loader, criterion)\n",
    "    res[lang_id]['unmasked']['logprobs'] = logprobs\n",
    "    res[lang_id]['unmasked']['targets'] = targets\n",
    "    res[lang_id]['unmasked']['indices'] = target_indices\n",
    "    \n",
    "    logprobs, target_indices, targets = test(model, test_loader_vowel, criterion)\n",
    "    res[lang_id]['vowel_masking']['logprobs'] = logprobs\n",
    "    res[lang_id]['vowel_masking']['targets'] = targets\n",
    "    res[lang_id]['vowel_masking']['indices'] = target_indices\n",
    "\n",
    "    logprobs, target_indices, targets = test(model, test_loader_consonant, criterion)\n",
    "    res[lang_id]['consonant_masking']['logprobs'] = logprobs\n",
    "    res[lang_id]['consonant_masking']['targets'] = targets\n",
    "    res[lang_id]['consonant_masking']['indices'] = target_indices\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fin\n",
      "Epoch\tLoss\tPerplexity\n",
      "1\t0.0635\t0\t\n",
      "2\t0.0613\t0\t\n",
      "3\t0.0601\t0\t\n",
      "\tPatience lost, remaining patience: 0\n",
      "4\t0.0595\t0\t\n",
      "Test loss: 0.05927572977934645\n",
      "Test perplexity: 0\n",
      "Test loss: 0.060217752283503886\n",
      "Test perplexity: 0\n",
      "Test loss: 0.060627393207639595\n",
      "Test perplexity: 0\n",
      "tur\n",
      "Epoch\tLoss\tPerplexity\n",
      "1\t0.107\t0\t\n",
      "\tPatience lost, remaining patience: 0\n",
      "2\t0.1084\t0\t\n",
      "Test loss: 0.10215801971820487\n",
      "Test perplexity: 0\n",
      "Test loss: 0.10011725981782794\n",
      "Test perplexity: 0\n",
      "Test loss: 0.10301283374120619\n",
      "Test perplexity: 0\n",
      "hye\n",
      "Epoch\tLoss\tPerplexity\n",
      "1\t0.0627\t0\t\n",
      "2\t0.0616\t0\t\n",
      "3\t0.0601\t0\t\n",
      "4\t0.0591\t0\t\n",
      "\tPatience lost, remaining patience: 0\n",
      "5\t0.0592\t0\t\n",
      "Test loss: 0.059826581312066404\n",
      "Test perplexity: 0\n",
      "Test loss: 0.05294920096654488\n",
      "Test perplexity: 0\n",
      "Test loss: 0.06236687182150109\n",
      "Test perplexity: 0\n",
      "arb\n",
      "Epoch\tLoss\tPerplexity\n",
      "1\t0.0619\t0\t\n",
      "2\t0.0565\t0\t\n",
      "\tPatience lost, remaining patience: 0\n",
      "3\t0.0615\t0\t\n",
      "Test loss: 0.03834541480234064\n",
      "Test perplexity: 0\n",
      "Test loss: 0.039504065947246814\n",
      "Test perplexity: 0\n",
      "Test loss: 0.04032134759018653\n",
      "Test perplexity: 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "save_results(base_path, datasets, res, criteria, models)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e1370a0257593aa7a1e6890fbe67f267b399c96ad7c3d275027d446a8086286"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mt': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}